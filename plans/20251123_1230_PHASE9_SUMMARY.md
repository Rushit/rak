# Phase 9: What Changes with CPU & Memory Profiling

**Quick Answer Summary**

---

## ğŸ¯ Core Question

**"What would change if we want to measure profile data on memory and CPU usage as well?"**

---

## ğŸ“ Short Answer

Adding CPU and memory profiling requires **4 major additions** to the Phase 9 observability system:

### 1. **New Profiling Module** (`zdk-telemetry/profiling/`)
   - CPU profiler using `pprof` format
   - Memory tracker with platform-specific hooks
   - Resource attribution per agent/session
   - Periodic profile collection

### 2. **New Dependencies**
   ```toml
   pprof = "0.13"              # CPU profiling
   dashmap = "5.5"             # Concurrent maps
   procfs = "0.16"             # Linux memory info
   libc = "0.2"                # macOS memory info
   ```

### 3. **New Export Protocol** (pprof format)
   - In addition to OTLP (for traces/metrics)
   - Export to: Pyroscope, GCP Cloud Profiler, or files
   - Correlate profiles with traces via `trace_id`

### 4. **Enhanced Configuration**
   ```toml
   [telemetry.profiling]
   enabled = true
   
   [telemetry.profiling.cpu]
   frequency = 100  # Hz
   
   [telemetry.profiling.memory]
   sampling_rate = 100  # Sample 1%
   ```

---

## ğŸ” Detailed Changes

### A. Data Collection Layer

**Before (Phase 9 without profiling)**:
- âœ… Traces (spans showing execution flow)
- âœ… Metrics (counters, histograms)

**After (Phase 9 with profiling)**:
- âœ… Traces
- âœ… Metrics
- ğŸ†• **CPU Profiles** (where CPU time goes)
- ğŸ†• **Memory Profiles** (allocation patterns)
- ğŸ†• **Resource Attribution** (cost per agent/session)

---

### B. Architecture Changes

```diff
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   zdk-telemetry                     â”‚
  â”‚                                     â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
  â”‚  â”‚ Traces  â”‚  â”‚ Metrics â”‚          â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
+ â”‚                                     â”‚
+ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
+ â”‚  â”‚   CPU   â”‚  â”‚ Memory  â”‚          â”‚
+ â”‚  â”‚ Profile â”‚  â”‚ Profile â”‚          â”‚
+ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚
           â–¼                â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Jaeger  â”‚    â”‚Pyroscope â”‚ â† NEW
     â”‚  (OTLP)  â”‚    â”‚  (pprof) â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### C. New Files Required

**Phase 9 without profiling**: ~8 files
```
zdk-telemetry/src/
â”œâ”€â”€ spans.rs (enhanced)
â”œâ”€â”€ exporters.rs (OTLP)
â”œâ”€â”€ sampling.rs
â”œâ”€â”€ context.rs
â””â”€â”€ metrics.rs
```

**Phase 9 WITH profiling**: +6 files (~34 hours additional work)
```
zdk-telemetry/src/
â”œâ”€â”€ ... (existing files)
â””â”€â”€ profiling/           â† NEW MODULE
    â”œâ”€â”€ mod.rs
    â”œâ”€â”€ cpu.rs           â† CPU profiling
    â”œâ”€â”€ memory.rs        â† Memory profiling
    â”œâ”€â”€ attribution.rs   â† Resource tracking
    â”œâ”€â”€ exporters.rs     â† pprof exporters
    â””â”€â”€ collector.rs     â† Periodic collection
```

---

### D. Integration Changes

#### Without Profiling:
```rust
impl Runner {
    pub async fn run(...) -> Result<Stream<Event>> {
        // Create trace span
        let span = trace_runner_start(...);
        
        // Run agent
        let events = agent.run(ctx).await;
        
        // Complete span
        span.complete(Ok(()));
        
        Ok(events)
    }
}
```

#### With Profiling:
```rust
impl Runner {
    pub async fn run(...) -> Result<Stream<Event>> {
        // Create trace span
        let span = trace_runner_start(...);
        
        // ğŸ†• Start resource tracking
        let start_memory = profiler.memory.stats().current_usage;
        let start_time = Instant::now();
        
        profiler.attribution.start_invocation(ResourceContext {
            invocation_id: inv_id.clone(),
            start_memory,
            ...
        });
        
        // Run agent
        let events = agent.run(ctx).await;
        
        // ğŸ†• Record resource usage
        let cpu_micros = start_time.elapsed().as_micros();
        let end_memory = profiler.memory.stats().current_usage;
        
        profiler.attribution.end_invocation(&inv_id, end_memory);
        
        // ğŸ†• Add resource attributes to span
        span.record("resource.cpu.micros", cpu_micros);
        span.record("resource.memory.bytes", end_memory - start_memory);
        
        // Complete span
        span.complete(Ok(()));
        
        Ok(events)
    }
}
```

---

### E. New Capabilities Enabled

| Capability | Without Profiling | With Profiling |
|------------|-------------------|----------------|
| **"Which agent is slow?"** | âœ… Via trace duration | âœ… Same |
| **"Why is it slow?"** | âŒ Unknown | âœ… See CPU flamegraph |
| **"Which function is hot?"** | âŒ | âœ… pprof shows exact functions |
| **"Is there a memory leak?"** | âŒ | âœ… Memory timeline shows growth |
| **"How much RAM per agent?"** | âŒ | âœ… Attribution per invocation |
| **"What's the cost?"** | âš ï¸ Duration only | âœ… CPU + Memory costs |

---

### F. Debugging Workflow Enhancement

**Without Profiling**:
1. Find slow trace in Jaeger: "agent.run took 8s"
2. See it called `tool.execute` which took 6s
3. ğŸ¤· **Can't tell why the tool is slow**

**With Profiling**:
1. Find slow trace in Jaeger: "agent.run took 8s"
2. See it called `tool.execute` which took 6s
3. Click "View Profile" â†’ jump to Pyroscope
4. ğŸ¯ **See flamegraph showing `regex::match` taking 60% of CPU**
5. ğŸ¯ **Fix the regex, optimize, redeploy**
6. âœ… **Verify P95 latency drops from 8s â†’ 1.5s**

---

### G. Performance Overhead

| Component | Overhead | Total |
|-----------|----------|-------|
| Tracing | 1-2% | 1-2% |
| Metrics | <1% | 2-3% |
| **+ CPU Profiling** | **1-3%** | **3-6%** |
| **+ Memory Profiling** | **2-5%** | **5-10%** |

**Mitigation**:
- Use sampling (100 Hz for CPU, 1% for memory)
- Disable in production if overhead too high
- Enable per-request with header flag

---

### H. Backend Options

**Without Profiling**:
- Traces â†’ Jaeger, Tempo, GCP Cloud Trace
- Metrics â†’ Prometheus

**With Profiling**:
- Traces â†’ Jaeger, Tempo, GCP Cloud Trace
- Metrics â†’ Prometheus
- **Profiles â†’ Pyroscope, GCP Cloud Profiler, Grafana Pyre** â† NEW

---

### I. Cost Implications

**Data Volume** (per 1000 agent runs/hour):

| Type | Without Profiling | With Profiling |
|------|-------------------|----------------|
| Traces | ~10 MB/hour | ~10 MB/hour |
| Metrics | ~500 KB/hour | ~500 KB/hour |
| **CPU Profiles** | - | **+6 MB/hour** |
| **Memory Snapshots** | - | **+60 KB/hour** |
| **Total** | **~11 MB/hour** | **~17 MB/hour (+55%)** |

**Cost**: ~$5-20/month for typical workload (depending on backend)

---

## ğŸ¯ Implementation Timeline

**Phase 9 Base** (without profiling): ~37 hours
- Distributed tracing: 22 hours
- Metrics: 8 hours
- Examples & docs: 7 hours

**Phase 9 Extended** (with profiling): +34 hours = **71 hours total**
- CPU profiling: 6 hours
- Memory profiling: 8 hours
- Resource attribution: 6 hours
- Profile exporters: 8 hours
- Integration & testing: 6 hours

**Total Difference**: +34 hours (~1 week additional work)

---

## âœ… Recommendation

### Start with Base (without profiling)?
âœ… **If**: You need observability quickly  
âœ… **If**: You're okay with "what happened" without "why slow"  
âŒ **If**: You need to optimize performance

### Start with Full (with profiling)?
âœ… **If**: Performance is critical  
âœ… **If**: You need cost attribution  
âœ… **If**: You're optimizing production systems  
âš ï¸ **If**: You can accept 5-10% overhead

### Recommended Approach
1. **Week 1-2**: Implement base Phase 9 (tracing + metrics)
2. **Week 2**: Test in development
3. **Week 3**: Add CPU profiling
4. **Week 4**: Add memory profiling
5. **Week 5**: Production rollout

This way, you get **quick wins** (tracing) followed by **deep insights** (profiling).

---

## ğŸ“š Key Documents

1. **20251123_1200_PHASE9_ADVANCED_OBSERVABILITY_DESIGN.md**  
   â†’ Complete Phase 9 design (base observability)

2. **20251123_1210_PROFILING_DESIGN.md**  
   â†’ CPU & memory profiling extension

3. **20251123_1220_COMPLETE_OBSERVABILITY_SYSTEM.md**  
   â†’ Full architecture with everything integrated

---

## ğŸš€ Bottom Line

**Adding CPU and memory profiling changes**:
- âœ… **+1 new module** (`profiling/`)
- âœ… **+6 new files** (~1500 lines of code)
- âœ… **+4 new dependencies** (pprof, dashmap, etc.)
- âœ… **+1 new backend** (Pyroscope or GCP Cloud Profiler)
- âœ… **+34 hours** of implementation time
- âœ… **+~50% data volume** (still small: ~17 MB/hour)
- âœ… **+3-5% performance overhead** (acceptable)

**Benefits**:
- ğŸ¯ **10x faster** root cause analysis
- ğŸ¯ **Exact function-level** bottleneck identification
- ğŸ¯ **Memory leak** detection
- ğŸ¯ **Cost attribution** per agent/session
- ğŸ¯ **Optimization guidance** with flamegraphs

**Worth it?** 

If you're building a **production agent system** that needs to be **fast and cost-efficient**, then **YES** - the profiling capabilities will pay for themselves in saved debugging time and optimized resource usage.

If you're building a **prototype or MVP**, start with **base Phase 9** (tracing + metrics) and **add profiling later** when you need to optimize.

